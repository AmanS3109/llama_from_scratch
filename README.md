# ğŸ”¥ LLaMA-Inspired Transformer Components (From Scratch)

This repository contains from-scratch implementations of core components used in large language models like Meta's **LLaMA** â€” written in pure PyTorch, without relying on any high-level libraries like HuggingFace Transformers.

## âœ¨ Features

- ğŸ§  **Custom Tokenizer** using Byte Pair Encoding (BPE)
- ğŸ§® **Self-Attention Module** with:
  - Multi-head & Grouped-Query Attention (GQA)
  - Rotary Position Embeddings (RoPE)
  - Optional Query/Key L2 Normalization
- âš¡ **Feed-Forward Network (FFN)** with:
  - RMSNorm normalization
  - SwiGLU activation (`SiLU(x) * Linear(x)`)
- ğŸ” Educational and minimal design â€” ideal for learning LLM internals
- âœ… All modules tested with standalone examples

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ tokenizer.py          # Custom BPE Tokenizer
â”œâ”€â”€ self_attention.py     # Self-Attention with GQA & RoPE
â”œâ”€â”€ feed_forward.py       # FFN with RMSNorm & SwiGLU
â””â”€â”€ README.md             # You're here!
```

---

## ğŸ”§ Requirements

- Python 3.8+
- PyTorch 1.13+ (or any recent version)

Install dependencies:

```bash
pip install torch
```

---

## ğŸ§ª Running the Examples

Each `.py` file is fully self-contained and includes a test snippet.

```bash
# Run tokenizer
python tokenizer.py

# Run attention
python self_attention.py

# Run feed-forward
python feed_forward.py
```

---

## ğŸ“š Concepts Covered

| Module            | Concepts Implemented                                                                 |
|-------------------|----------------------------------------------------------------------------------------|
| `tokenizer.py`     | Byte Pair Encoding (BPE), Vocabulary merging, Token ID mapping                       |
| `self_attention.py`| Multi-head Attention, Grouped-Query Attention (GQA), RoPE, Causal Masking             |
| `feed_forward.py`  | RMSNorm, SwiGLU activation, Intermediate projections, Residual pipeline              |

---

## ğŸš€ What's Next?

Stay tuned for the next steps:
- âœ… Combine into a `TransformerBlock`
- ğŸ”œ Stack blocks into a full `LLaMA-style LLM`
- ğŸ”œ Add embeddings, causal language modeling, and training loop

---

## ğŸ“„ License

This project is released under the [MIT License](LICENSE).

---

## ğŸ™Œ Acknowledgements

Inspired by:
- Meta AI's LLaMA and LLaMA-2/3 papers
- nanoGPT, llama.c, and Transformer lens
- Deep dive videos and blogs from Karpathy, Yannic Kilcher, and others

---

## ğŸ’¬ Author

**Aman Singh**  
_Transforming theory into working ML systems from scratch._

Let's connect: [LinkedIn](https://www.linkedin.com) â€¢ [GitHub](https://github.com)
